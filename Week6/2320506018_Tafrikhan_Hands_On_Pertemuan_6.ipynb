{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "f89f7684",
      "metadata": {
        "id": "f89f7684"
      },
      "source": [
        "# Hands-On Pertemuan 6: Data Processing dengan Apache Spark"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e30ce9d1",
      "metadata": {
        "id": "e30ce9d1"
      },
      "source": [
        "## Tujuan:\n",
        "- Memahami dan mempraktikkan data processing menggunakan Apache Spark.\n",
        "- Menggunakan Spark untuk operasi data yang efisien pada dataset besar.\n",
        "- Menerapkan teknik canggih dalam Spark untuk mengatasi kasus penggunaan nyata."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cd5c2f90",
      "metadata": {
        "id": "cd5c2f90"
      },
      "source": [
        "### 1. Pengenalan Spark DataFrames\n",
        "Spark DataFrame menyediakan struktur data yang optimal dengan operasi yang dioptimalkan untuk pemrosesan data besar, yang sangat mirip dengan DataFrame di Pandas atau di RDBMS.\n",
        "\n",
        "- **Tugas 1**: Buat DataFrame sederhana di Spark dan eksplorasi beberapa fungsi dasar yang tersedia."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Pertama kita install pyspark\n",
        "!pip install pyspark\n",
        "!pip install pandas\n",
        "!pip install matplotlib"
      ],
      "metadata": {
        "id": "cxI4DYvozYyN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad759cd1-9c81-4786-ebb6-94b0b03ed2ee"
      },
      "id": "cxI4DYvozYyN",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.3.tar.gz (317.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.3/317.3 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.3-py2.py3-none-any.whl size=317840625 sha256=13e43a56fb8ef4dda0e46121bdd8b0480b72c58d95d538f9bec904ca4aa4a1c3\n",
            "  Stored in directory: /root/.cache/pip/wheels/1b/3a/92/28b93e2fbfdbb07509ca4d6f50c5e407f48dce4ddbda69a4ab\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.3\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.1.4)\n",
            "Requirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.7)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (10.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "986d01c7",
      "metadata": {
        "id": "986d01c7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6fb633be-b3a7-4457-8060-32d4beabf9bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+----------+------+\n",
            "|EmployeeName|Department|Salary|\n",
            "+------------+----------+------+\n",
            "|       James|     Sales|  3000|\n",
            "|     Michael|     Sales|  4600|\n",
            "|      Robert|     Sales|  4100|\n",
            "|       Maria|   Finance|  3000|\n",
            "+------------+----------+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Contoh membuat DataFrame sederhana dan operasi dasar\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName('HandsOnPertemuan6').getOrCreate()\n",
        "\n",
        "data = [('James', 'Sales', 3000),\n",
        "        ('Michael', 'Sales', 4600),\n",
        "        ('Robert', 'Sales', 4100),\n",
        "        ('Maria', 'Finance', 3000)]\n",
        "columns = ['EmployeeName', 'Department', 'Salary']\n",
        "\n",
        "df = spark.createDataFrame(data, schema=columns)\n",
        "df.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fca66b73",
      "metadata": {
        "id": "fca66b73"
      },
      "source": [
        "### 2. Transformasi Dasar dengan DataFrames\n",
        "Pemrosesan data meliputi transformasi seperti filtering, selections, dan aggregations. Spark menyediakan cara efisien untuk melaksanakan operasi ini.\n",
        "\n",
        "- **Tugas 2**: Gunakan operasi filter, select, groupBy untuk mengekstrak informasi dari data, serta lakukan agregasi data untuk mendapatkan insight tentang dataset menggunakan perintah seperti mean, max, sum."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "58232678",
      "metadata": {
        "id": "58232678",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c3c9514-0cd5-4e12-afd2-e39d14801d4e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+------+\n",
            "|EmployeeName|Salary|\n",
            "+------------+------+\n",
            "|       James|  3000|\n",
            "|     Michael|  4600|\n",
            "|      Robert|  4100|\n",
            "|       Maria|  3000|\n",
            "+------------+------+\n",
            "\n",
            "+------------+----------+------+\n",
            "|EmployeeName|Department|Salary|\n",
            "+------------+----------+------+\n",
            "|     Michael|     Sales|  4600|\n",
            "|      Robert|     Sales|  4100|\n",
            "+------------+----------+------+\n",
            "\n",
            "+----------+-----------+\n",
            "|Department|avg(Salary)|\n",
            "+----------+-----------+\n",
            "|     Sales|     3900.0|\n",
            "|   Finance|     3000.0|\n",
            "+----------+-----------+\n",
            "\n",
            "+----------+-----------+\n",
            "|Department|max(Salary)|\n",
            "+----------+-----------+\n",
            "|     Sales|       4600|\n",
            "|   Finance|       3000|\n",
            "+----------+-----------+\n",
            "\n",
            "+----------+-----------+\n",
            "|Department|sum(Salary)|\n",
            "+----------+-----------+\n",
            "|     Sales|      11700|\n",
            "|   Finance|       3000|\n",
            "+----------+-----------+\n",
            "\n",
            "+----------+-----+\n",
            "|Department|count|\n",
            "+----------+-----+\n",
            "|     Sales|    3|\n",
            "|   Finance|    1|\n",
            "+----------+-----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Contoh operasi:\n",
        "df.select('EmployeeName', 'Salary').show()\n",
        "df.filter(df['Salary'] > 3000).show()\n",
        "df.groupBy('Department').avg('Salary').show()\n",
        "\n",
        "# Contoh lainnya:\n",
        "\n",
        "# nilai maksimal dari kolom Salary\n",
        "df.groupBy('Department').max('Salary').show()\n",
        "\n",
        "# total Salary untuk setiap Department\n",
        "df.groupBy('Department').sum('Salary').show()\n",
        "\n",
        "# jumlah karyawan di setiap Department\n",
        "df.groupBy('Department').count().show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "89763d36",
      "metadata": {
        "id": "89763d36"
      },
      "source": [
        "### 3. Bekerja dengan Tipe Data Kompleks\n",
        "Spark mendukung tipe data yang kompleks seperti maps, arrays, dan structs yang memungkinkan operasi yang lebih kompleks pada dataset yang kompleks.\n",
        "\n",
        "- **Tugas 3**: Eksplorasi bagaimana mengolah tipe data kompleks dalam Spark DataFrames."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F\n",
        "\n",
        "# Membuat kolom SalaryBonus\n",
        "df_with_bonus = df.withColumn('SalaryBonus', df['Salary'] * 0.1)\n",
        "\n",
        "# Membuat TotalCompensation\n",
        "df_with_total_compensation = df_with_bonus.withColumn('TotalCompensation', df_with_bonus['Salary'] + df_with_bonus['SalaryBonus'])\n",
        "\n",
        "# Output\n",
        "df_with_total_compensation.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y-61OFPi31DB",
        "outputId": "e521a83e-a1b0-4569-eee4-ad98b92c893a"
      },
      "id": "Y-61OFPi31DB",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+----------+------+-----------+-----------------+\n",
            "|EmployeeName|Department|Salary|SalaryBonus|TotalCompensation|\n",
            "+------------+----------+------+-----------+-----------------+\n",
            "|       James|     Sales|  3000|      300.0|           3300.0|\n",
            "|     Michael|     Sales|  4600|      460.0|           5060.0|\n",
            "|      Robert|     Sales|  4100|      410.0|           4510.0|\n",
            "|       Maria|   Finance|  3000|      300.0|           3300.0|\n",
            "+------------+----------+------+-----------+-----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5b3b58dd",
      "metadata": {
        "id": "5b3b58dd"
      },
      "source": [
        "### 4. Operasi Data Lanjutan\n",
        "Menggunakan Spark untuk operasi lanjutan seperti window functions, user-defined functions (UDFs), dan mengoptimalkan query.\n",
        "\n",
        "- **Tugas 4**: Implementasikan window function untuk menghitung running totals atau rangkings."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "windowSpec = Window.partitionBy('Department').orderBy('Salary').rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
        "\n",
        "# Membuat rank dan running total\n",
        "df_with_running_total = df.withColumn('Rank', F.rank().over(windowSpec)) \\\n",
        "                          .withColumn('RunningTotal', F.sum('Salary').over(windowSpec))\n",
        "\n",
        "df_with_running_total.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c91ZL7NNG9bI",
        "outputId": "8e520c72-fe84-406f-fcd4-3042dd72be56"
      },
      "id": "c91ZL7NNG9bI",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+----------+------+----+------------+\n",
            "|EmployeeName|Department|Salary|Rank|RunningTotal|\n",
            "+------------+----------+------+----+------------+\n",
            "|       Maria|   Finance|  3000|   1|        3000|\n",
            "|       James|     Sales|  3000|   1|        3000|\n",
            "|      Robert|     Sales|  4100|   2|        7100|\n",
            "|     Michael|     Sales|  4600|   3|       11700|\n",
            "+------------+----------+------+----+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f8a097ec",
      "metadata": {
        "id": "f8a097ec"
      },
      "source": [
        "### 5. Kesimpulan dan Eksplorasi Lebih Lanjut\n",
        "Review apa yang telah dipelajari tentang pemrosesan data menggunakan Spark dan eksplorasi teknik lebih lanjut untuk mengoptimalkan pemrosesan data Anda.\n",
        "- **Tugas 5**: Buat ringkasan dari semua operasi yang telah dilakukan dan bagaimana teknik ini dapat diterapkan pada proyek data Anda."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Jawaban nomor 5 ada di Medium saya.**"
      ],
      "metadata": {
        "id": "_wS3L9hhJdk5"
      },
      "id": "_wS3L9hhJdk5"
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}