{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "e2a030e3",
      "metadata": {
        "id": "e2a030e3"
      },
      "source": [
        "# Hands-On Pertemuan 14: Advanced Machine Learning using Spark MLlib"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "099562db",
      "metadata": {
        "id": "099562db"
      },
      "source": [
        "## Objectives:\n",
        "- Understand and implement advanced machine learning tasks using Spark MLlib.\n",
        "- Build and evaluate models using real-world datasets.\n",
        "- Explore techniques like feature engineering and hyperparameter tuning.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "77df771a",
      "metadata": {
        "id": "77df771a"
      },
      "source": [
        "## Introduction to Spark MLlib\n",
        "Spark MLlib is a scalable library for machine learning that integrates seamlessly with the Spark ecosystem. It supports a wide range of tasks, including regression, classification, clustering, and collaborative filtering."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark\n",
        "!pip install findspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Q3423jy918A",
        "outputId": "5d2b3111-77b7-4200-cf6a-e0aa5c714d62"
      },
      "id": "5Q3423jy918A",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.3)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Collecting findspark\n",
            "  Downloading findspark-2.0.1-py2.py3-none-any.whl.metadata (352 bytes)\n",
            "Downloading findspark-2.0.1-py2.py3-none-any.whl (4.4 kB)\n",
            "Installing collected packages: findspark\n",
            "Successfully installed findspark-2.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d9ae225b",
      "metadata": {
        "id": "d9ae225b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77906089-8f85-422e-de73-ec2a83d2737f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Coefficients: [0.9999999999999992]\n",
            "Intercept: 15.000000000000009\n"
          ]
        }
      ],
      "source": [
        "# Example: Linear Regression with Spark MLlib\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.regression import LinearRegression\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "\n",
        "# Initialize Spark Session\n",
        "spark = SparkSession.builder.appName('MLlib Example').getOrCreate()\n",
        "\n",
        "# Load sample data\n",
        "data = [(1, 5.0, 20.0), (2, 10.0, 25.0), (3, 15.0, 30.0), (4, 20.0, 35.0)]\n",
        "columns = ['ID', 'Feature', 'Target']\n",
        "df = spark.createDataFrame(data, columns)\n",
        "\n",
        "# Prepare data for modeling\n",
        "assembler = VectorAssembler(inputCols=['Feature'], outputCol='Features')\n",
        "df_transformed = assembler.transform(df)\n",
        "\n",
        "# Train a linear regression model\n",
        "lr = LinearRegression(featuresCol='Features', labelCol='Target')\n",
        "model = lr.fit(df_transformed)\n",
        "\n",
        "# Print model coefficients\n",
        "print(f'Coefficients: {model.coefficients}')\n",
        "print(f'Intercept: {model.intercept}')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml.linalg import Vectors\n",
        "\n",
        "# Example dataset\n",
        "data = [(1, Vectors.dense([2.0, 3.0]), 0),\n",
        "        (2, Vectors.dense([1.0, 5.0]), 1),\n",
        "        (3, Vectors.dense([2.5, 4.5]), 1),\n",
        "        (4, Vectors.dense([3.0, 6.0]), 0)]\n",
        "columns = ['ID', 'Features', 'Label']\n",
        "df = spark.createDataFrame(data, columns)\n",
        "\n",
        "# Train logistic regression model\n",
        "lr = LogisticRegression(featuresCol='Features', labelCol='Label')\n",
        "model = lr.fit(df)\n",
        "\n",
        "# Display coefficients and summary\n",
        "print(f'Coefficients: {model.coefficients}')\n",
        "print(f'Intercept: {model.intercept}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Py05MlowBebM",
        "outputId": "4aa0fd6a-4a60-4c21-aee8-1d6ff697d4c0"
      },
      "id": "Py05MlowBebM",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Coefficients: [-12.262057929180484,4.087352266486688]\n",
            "Intercept: 11.56891272665312\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Practice: KMeans Clustering\n",
        "from pyspark.ml.clustering import KMeans\n",
        "from pyspark.ml.linalg import Vectors # Import Vectors\n",
        "\n",
        "# Example dataset\n",
        "data = [(1, Vectors.dense([1.0, 1.0])),\n",
        "        (2, Vectors.dense([5.0, 5.0])),\n",
        "        (3, Vectors.dense([10.0, 10.0])),\n",
        "        (4, Vectors.dense([15.0, 15.0]))]\n",
        "columns = ['ID', 'Features']\n",
        "df = spark.createDataFrame(data, columns)\n",
        "\n",
        "# Train KMeans clustering model\n",
        "kmeans = KMeans(featuresCol='Features', k=2)\n",
        "model = kmeans.fit(df)\n",
        "\n",
        "# Show cluster centers\n",
        "centers = model.clusterCenters()\n",
        "print(f'Cluster Centers: {centers}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hVNTd9K9Cyoh",
        "outputId": "6cf3ffc3-4044-406e-f920-084ec31fc1f8"
      },
      "id": "hVNTd9K9Cyoh",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cluster Centers: [array([5.33333333, 5.33333333]), array([15., 15.])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a60a8d7e",
      "metadata": {
        "id": "a60a8d7e"
      },
      "source": [
        "## Homework\n",
        "- Load a real-world dataset into Spark and prepare it for machine learning tasks.\n",
        "- Build a classification model using Spark MLlib and evaluate its performance.\n",
        "- Explore hyperparameter tuning using cross-validation.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark"
      ],
      "metadata": {
        "id": "-d2y8Jeairvs"
      },
      "id": "-d2y8Jeairvs",
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName('weather_pred').getOrCreate()"
      ],
      "metadata": {
        "id": "bA-x3yTcdJSl"
      },
      "id": "bA-x3yTcdJSl",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1 = spark.read.csv('weather_prediction_dataset.csv', header=True, inferSchema=True)\n",
        "df2 = spark.read.csv('weather_prediction_bbq_labels.csv',header=True, inferSchema=True)"
      ],
      "metadata": {
        "id": "3pGzXQDJdJyd"
      },
      "id": "3pGzXQDJdJyd",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1.columns[80:91]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wEaOWO5edRet",
        "outputId": "653d0a99-a61f-48d3-b6d2-08f17a94e85e"
      },
      "id": "wEaOWO5edRet",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['MAASTRICHT_cloud_cover',\n",
              " 'MAASTRICHT_wind_speed',\n",
              " 'MAASTRICHT_wind_gust',\n",
              " 'MAASTRICHT_humidity',\n",
              " 'MAASTRICHT_pressure',\n",
              " 'MAASTRICHT_global_radiation',\n",
              " 'MAASTRICHT_precipitation',\n",
              " 'MAASTRICHT_sunshine',\n",
              " 'MAASTRICHT_temp_mean',\n",
              " 'MAASTRICHT_temp_min',\n",
              " 'MAASTRICHT_temp_max']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dfM = df1.select(['DATE','MAASTRICHT_cloud_cover',\n",
        " 'MAASTRICHT_wind_speed',\n",
        " 'MAASTRICHT_wind_gust',\n",
        " 'MAASTRICHT_humidity',\n",
        " 'MAASTRICHT_pressure',\n",
        " 'MAASTRICHT_global_radiation',\n",
        " 'MAASTRICHT_precipitation',\n",
        " 'MAASTRICHT_sunshine',\n",
        " 'MAASTRICHT_temp_mean',\n",
        " 'MAASTRICHT_temp_min',\n",
        " 'MAASTRICHT_temp_max'])"
      ],
      "metadata": {
        "id": "9rTnRsWsdeMz"
      },
      "id": "9rTnRsWsdeMz",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dfM2 = df2.select(['DATE','MAASTRICHT_BBQ_weather'])\n",
        "dfM2.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CsAGhiIPdgFZ",
        "outputId": "20048b33-b9f1-41bc-e8e8-bd7ad60f7e3e"
      },
      "id": "CsAGhiIPdgFZ",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+----------------------+\n",
            "|    DATE|MAASTRICHT_BBQ_weather|\n",
            "+--------+----------------------+\n",
            "|20000101|                 false|\n",
            "|20000102|                 false|\n",
            "|20000103|                 false|\n",
            "|20000104|                 false|\n",
            "|20000105|                 false|\n",
            "|20000106|                 false|\n",
            "|20000107|                 false|\n",
            "|20000108|                 false|\n",
            "|20000109|                 false|\n",
            "|20000110|                 false|\n",
            "|20000111|                 false|\n",
            "|20000112|                 false|\n",
            "|20000113|                 false|\n",
            "|20000114|                 false|\n",
            "|20000115|                 false|\n",
            "|20000116|                 false|\n",
            "|20000117|                 false|\n",
            "|20000118|                 false|\n",
            "|20000119|                 false|\n",
            "|20000120|                 false|\n",
            "+--------+----------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark.sql.functions as F\n",
        "from functools import reduce\n",
        "\n",
        "#convert the Boolean True/Fase to 0 or 1\n",
        "cols = [\"MAASTRICHT_BBQ_weather\"]\n",
        "dfM2 = reduce(lambda df, c: dfM2.withColumn(c, F.when(df[c] == 'false', 0).otherwise(1)), cols, dfM2)"
      ],
      "metadata": {
        "id": "e1h-Xdd0di4F"
      },
      "id": "e1h-Xdd0di4F",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Join the labels (True, False) from the second newly created dataframe to the first one\n",
        "dfM2= dfM2.join(dfM, on=[\"DATE\"])\n",
        "dfM2.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "38t67m7xdnxJ",
        "outputId": "79dddd04-9622-47f6-95ad-b1a20a92d36a"
      },
      "id": "38t67m7xdnxJ",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- DATE: integer (nullable = true)\n",
            " |-- MAASTRICHT_BBQ_weather: integer (nullable = false)\n",
            " |-- MAASTRICHT_cloud_cover: integer (nullable = true)\n",
            " |-- MAASTRICHT_wind_speed: double (nullable = true)\n",
            " |-- MAASTRICHT_wind_gust: double (nullable = true)\n",
            " |-- MAASTRICHT_humidity: double (nullable = true)\n",
            " |-- MAASTRICHT_pressure: double (nullable = true)\n",
            " |-- MAASTRICHT_global_radiation: double (nullable = true)\n",
            " |-- MAASTRICHT_precipitation: double (nullable = true)\n",
            " |-- MAASTRICHT_sunshine: double (nullable = true)\n",
            " |-- MAASTRICHT_temp_mean: double (nullable = true)\n",
            " |-- MAASTRICHT_temp_min: double (nullable = true)\n",
            " |-- MAASTRICHT_temp_max: double (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Convert data to Spark ML format\n",
        "from pyspark.ml.feature import VectorAssembler"
      ],
      "metadata": {
        "id": "eA7lNN84d9i5"
      },
      "id": "eA7lNN84d9i5",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#create an instance of the Assembler which takes in a series of columns to be used as features\n",
        "#and returns a condensed vector\n",
        "assembler = VectorAssembler(inputCols = ['DATE','MAASTRICHT_cloud_cover',\n",
        " 'MAASTRICHT_wind_speed',\n",
        " 'MAASTRICHT_wind_gust',\n",
        " 'MAASTRICHT_humidity',\n",
        " 'MAASTRICHT_pressure',\n",
        " 'MAASTRICHT_global_radiation',\n",
        " 'MAASTRICHT_precipitation',\n",
        " 'MAASTRICHT_sunshine',\n",
        " 'MAASTRICHT_temp_mean',\n",
        " 'MAASTRICHT_temp_min',\n",
        " 'MAASTRICHT_temp_max',\n",
        " 'MAASTRICHT_cloud_cover',\n",
        " 'MAASTRICHT_wind_speed',\n",
        " 'MAASTRICHT_wind_gust',\n",
        " 'MAASTRICHT_humidity',\n",
        " 'MAASTRICHT_pressure',\n",
        " 'MAASTRICHT_global_radiation',\n",
        " 'MAASTRICHT_precipitation',\n",
        " 'MAASTRICHT_sunshine',\n",
        " 'MAASTRICHT_temp_mean',\n",
        " 'MAASTRICHT_temp_min',\n",
        " 'MAASTRICHT_temp_max'], outputCol = 'features')"
      ],
      "metadata": {
        "id": "ncJZa5bNeA75"
      },
      "id": "ncJZa5bNeA75",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Call the 'transform' method on the dataframe returns a new dataframe with the newly created 'features' column\n",
        "\n",
        "data = assembler.transform(dfM2)"
      ],
      "metadata": {
        "id": "75vaFshIeMht"
      },
      "id": "75vaFshIeMht",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_data = data.select(['features','MAASTRICHT_BBQ_weather'])\n",
        "final_data.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RR6Tsj7meOYM",
        "outputId": "e76f5a8e-80c7-4db2-9e5c-b02e9a3b6964"
      },
      "id": "RR6Tsj7meOYM",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+----------------------+\n",
            "|            features|MAASTRICHT_BBQ_weather|\n",
            "+--------------------+----------------------+\n",
            "|[2.0000101E7,8.0,...|                     0|\n",
            "|[2.0000102E7,7.0,...|                     0|\n",
            "|[2.0000103E7,7.0,...|                     0|\n",
            "|[2.0000104E7,8.0,...|                     0|\n",
            "|[2.0000105E7,4.0,...|                     0|\n",
            "|[2.0000106E7,6.0,...|                     0|\n",
            "|[2.0000107E7,6.0,...|                     0|\n",
            "|[2.0000108E7,7.0,...|                     0|\n",
            "|[2.0000109E7,6.0,...|                     0|\n",
            "|[2.000011E7,7.0,1...|                     0|\n",
            "|[2.0000111E7,3.0,...|                     0|\n",
            "|[2.0000112E7,5.0,...|                     0|\n",
            "|[2.0000113E7,8.0,...|                     0|\n",
            "|[2.0000114E7,8.0,...|                     0|\n",
            "|[2.0000115E7,8.0,...|                     0|\n",
            "|[2.0000116E7,8.0,...|                     0|\n",
            "|[2.0000117E7,8.0,...|                     0|\n",
            "|[2.0000118E7,8.0,...|                     0|\n",
            "|[2.0000119E7,7.0,...|                     0|\n",
            "|[2.000012E7,8.0,3...|                     0|\n",
            "+--------------------+----------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Machine Learning"
      ],
      "metadata": {
        "id": "GT5XRssGeU1p"
      },
      "id": "GT5XRssGeU1p"
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.classification import DecisionTreeClassifier,GBTClassifier,RandomForestClassifier\n",
        "from pyspark.ml import Pipeline"
      ],
      "metadata": {
        "id": "_G4cOatoebqh"
      },
      "id": "_G4cOatoebqh",
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data,test_data = final_data.randomSplit([0.7,0.3])"
      ],
      "metadata": {
        "id": "7P8ngW3fee2R"
      },
      "id": "7P8ngW3fee2R",
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dtc = DecisionTreeClassifier(labelCol='MAASTRICHT_BBQ_weather',featuresCol='features')\n",
        "rfc = RandomForestClassifier(labelCol='MAASTRICHT_BBQ_weather',featuresCol='features')\n",
        "gbt = GBTClassifier(labelCol='MAASTRICHT_BBQ_weather',featuresCol='features')"
      ],
      "metadata": {
        "id": "OTY0KEtPegvj"
      },
      "id": "OTY0KEtPegvj",
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the models\n",
        "dtc_model = dtc.fit(train_data)\n",
        "rfc_model = rfc.fit(train_data)\n",
        "gbt_model = gbt.fit(train_data)"
      ],
      "metadata": {
        "id": "XSEF63xdejAR"
      },
      "id": "XSEF63xdejAR",
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Call 'transform' on the test_data\n",
        "#these will create 3 new dataframes for each model\n",
        "dtc_predictions = dtc_model.transform(test_data)\n",
        "rfc_predictions = rfc_model.transform(test_data)\n",
        "gbt_predictions = gbt_model.transform(test_data)"
      ],
      "metadata": {
        "id": "x5Co_xKSepW_"
      },
      "id": "x5Co_xKSepW_",
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#let's look at one of the newly created dataframes\n",
        "rfc_predictions.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6WsHr0Zxesvn",
        "outputId": "2c431d63-1943-4e13-a2ef-12bdcb3e2a30"
      },
      "id": "6WsHr0Zxesvn",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+----------------------+--------------------+--------------------+----------+\n",
            "|            features|MAASTRICHT_BBQ_weather|       rawPrediction|         probability|prediction|\n",
            "+--------------------+----------------------+--------------------+--------------------+----------+\n",
            "|[2.0000101E7,8.0,...|                     0|[19.9992581602373...|[0.99996290801186...|       0.0|\n",
            "|[2.0000114E7,8.0,...|                     0|[19.9928479038271...|[0.99964239519135...|       0.0|\n",
            "|[2.0000116E7,8.0,...|                     0|[19.9992581602373...|[0.99996290801186...|       0.0|\n",
            "|[2.0000117E7,8.0,...|                     0|[19.9992581602373...|[0.99996290801186...|       0.0|\n",
            "|[2.0000122E7,7.0,...|                     0|[19.9992581602373...|[0.99996290801186...|       0.0|\n",
            "|[2.0000204E7,8.0,...|                     0|[19.9992581602373...|[0.99996290801186...|       0.0|\n",
            "|[2.0000206E7,7.0,...|                     0|[19.9992581602373...|[0.99996290801186...|       0.0|\n",
            "|[2.0000207E7,7.0,...|                     0|[19.9992581602373...|[0.99996290801186...|       0.0|\n",
            "|[2.0000213E7,5.0,...|                     0|[19.9992581602373...|[0.99996290801186...|       0.0|\n",
            "|[2.0000214E7,4.0,...|                     0|[19.9992581602373...|[0.99996290801186...|       0.0|\n",
            "|[2.0000218E7,8.0,...|                     0|[19.9992581602373...|[0.99996290801186...|       0.0|\n",
            "|[2.000022E7,4.0,2...|                     0|[19.9992581602373...|[0.99996290801186...|       0.0|\n",
            "|[2.0000223E7,7.0,...|                     0|[19.9928479038271...|[0.99964239519135...|       0.0|\n",
            "|[2.0000225E7,4.0,...|                     0|[19.9992581602373...|[0.99996290801186...|       0.0|\n",
            "|[2.0000301E7,7.0,...|                     0|[19.9992581602373...|[0.99996290801186...|       0.0|\n",
            "|[2.000031E7,8.0,4...|                     0|[19.9992581602373...|[0.99996290801186...|       0.0|\n",
            "|[2.0000313E7,7.0,...|                     0|[19.9928479038271...|[0.99964239519135...|       0.0|\n",
            "|[2.0000315E7,6.0,...|                     0|[19.9992581602373...|[0.99996290801186...|       0.0|\n",
            "|[2.0000319E7,7.0,...|                     0|[19.9992581602373...|[0.99996290801186...|       0.0|\n",
            "|[2.0000323E7,6.0,...|                     0|[19.9992581602373...|[0.99996290801186...|       0.0|\n",
            "+--------------------+----------------------+--------------------+--------------------+----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Accuracy Evaluation"
      ],
      "metadata": {
        "id": "gzQlUN_PevHf"
      },
      "id": "gzQlUN_PevHf"
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
      ],
      "metadata": {
        "id": "oBiAceB9e0ZD"
      },
      "id": "oBiAceB9e0ZD",
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Select (prediction, true label) and compute test error\n",
        "acc_evaluator = MulticlassClassificationEvaluator(labelCol=\"MAASTRICHT_BBQ_weather\", predictionCol=\"prediction\", metricName=\"accuracy\")"
      ],
      "metadata": {
        "id": "tGOunu8Xe3qu"
      },
      "id": "tGOunu8Xe3qu",
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dtc_acc = acc_evaluator.evaluate(dtc_predictions)\n",
        "rfc_acc = acc_evaluator.evaluate(rfc_predictions)\n",
        "gbt_acc = acc_evaluator.evaluate(gbt_predictions)"
      ],
      "metadata": {
        "id": "Ti_22olge6JJ"
      },
      "id": "Ti_22olge6JJ",
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('-'*80)\n",
        "print('A single decision tree had an accuracy of: {0:2.2f}%'.format(dtc_acc*100))\n",
        "print('-'*80)\n",
        "print('A random forest ensemble had an accuracy of: {0:2.2f}%'.format(rfc_acc*100))\n",
        "print('-'*80)\n",
        "print('A ensemble using GBT had an accuracy of: {0:2.2f}%'.format(gbt_acc*100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "649JaobWe8cd",
        "outputId": "74f756eb-6430-4b2f-e79c-040beba17f93"
      },
      "id": "649JaobWe8cd",
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "A single decision tree had an accuracy of: 99.39%\n",
            "--------------------------------------------------------------------------------\n",
            "A random forest ensemble had an accuracy of: 99.47%\n",
            "--------------------------------------------------------------------------------\n",
            "A ensemble using GBT had an accuracy of: 99.39%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# hyperparameter tuning cross-validation"
      ],
      "metadata": {
        "id": "3htFNjEEnhwQ"
      },
      "id": "3htFNjEEnhwQ"
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
        "\n",
        "# --- Hyperparameter Tuning untuk RandomForestClassifier ---\n",
        "\n",
        "# Definisikan parameter grid\n",
        "paramGrid_rfc = ParamGridBuilder() \\\n",
        "    .addGrid(rfc.numTrees, [10, 20, 30]) \\\n",
        "    .addGrid(rfc.maxDepth, [5, 10, 15]) \\\n",
        "    .build()\n",
        "\n",
        "# Buat objek CrossValidator\n",
        "crossval_rfc = CrossValidator(estimator=rfc,\n",
        "                              estimatorParamMaps=paramGrid_rfc,\n",
        "                              evaluator=acc_evaluator,\n",
        "                              numFolds=3)\n",
        "\n",
        "# Latih model\n",
        "cvModel_rfc = crossval_rfc.fit(train_data)\n",
        "\n",
        "\n",
        "\n",
        "# --- Hyperparameter Tuning untuk DecisionTreeClassifier ---\n",
        "\n",
        "# Definisikan parameter grid\n",
        "paramGrid_dtc = ParamGridBuilder() \\\n",
        "    .addGrid(dtc.maxDepth, [5, 10, 15]) \\\n",
        "    .addGrid(dtc.minInstancesPerNode, [1, 5, 10]) \\\n",
        "    .build()\n",
        "\n",
        "# Buat objek CrossValidator\n",
        "crossval_dtc = CrossValidator(estimator=dtc,\n",
        "                              estimatorParamMaps=paramGrid_dtc,\n",
        "                              evaluator=acc_evaluator,\n",
        "                              numFolds=3)\n",
        "\n",
        "# Latih model\n",
        "cvModel_dtc = crossval_dtc.fit(train_data)\n",
        "\n",
        "\n",
        "\n",
        "# --- Hyperparameter Tuning untuk GBTClassifier ---\n",
        "\n",
        "# Definisikan parameter grid\n",
        "paramGrid_gbt = ParamGridBuilder() \\\n",
        "    .addGrid(gbt.maxDepth, [5, 10, 15]) \\\n",
        "    .addGrid(gbt.maxIter, [10, 20, 30]) \\\n",
        "    .build()\n",
        "\n",
        "# Buat objek CrossValidator\n",
        "crossval_gbt = CrossValidator(estimator=gbt,\n",
        "                              estimatorParamMaps=paramGrid_gbt,\n",
        "                              evaluator=acc_evaluator,\n",
        "                              numFolds=3)\n",
        "\n",
        "# Latih model\n",
        "cvModel_gbt = crossval_gbt.fit(train_data)"
      ],
      "metadata": {
        "id": "tPy9XQiOoHEr"
      },
      "id": "tPy9XQiOoHEr",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}